{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86947d1-6ac3-41d6-862e-586f4ab55b15",
   "metadata": {},
   "source": [
    "### Download the Fashion_MNIST dataset by using PyTorch's torchvision.datasets module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dbf8e-9caa-40f8-8eae-ba86901e2cce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dec4e0-e311-428a-8227-ceb1f338314d",
   "metadata": {},
   "source": [
    "### Forword pass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e23381-c60f-4852-a401-19494c158611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation, lambda_reg=0.1, reg_type=None,dropout_rate=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(input_dim, output_dim)* 0.01\n",
    "        self.biases =  np.zeros((1, output_dim))\n",
    "        self.activation_name =activation\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "        self.reg_type = reg_type\n",
    "        self.dropout_mask = None\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "            self.activation_prime = self.relu_prime\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "            self.activation_prime = self.sigmoid_prime\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = self.softmax            \n",
    "        else:\n",
    "            raise ValueError('activation function is not defined')\n",
    "            \n",
    "    def __str__(self):\n",
    "        return f\"\"\"DenseLayer(input_dim:{self.input_dim}, output_dim:{self.output_dim}, activation:{self.activation_name})\"\"\"\n",
    "        \n",
    "    def forward(self, input_data,is_training = False):\n",
    "        self.input = input_data\n",
    "        #print(f\"self.input: {self.input.shape} \\n self.weights {self.weights.shape}\")\n",
    "        Z = np.dot(self.input, self.weights) + self.biases\n",
    "        #print(\"Z \", Z.shape)\n",
    "        if is_training and self.dropout_rate:\n",
    "            output_before_dropout = self.activation(Z)\n",
    "            self.dropout_mask = np.random.rand(*output_before_dropout.shape) > self.dropout_rate\n",
    "            #self.dropout_mask = np.random.binomial(1, 1- dropout_rate, size=output_before_dropout.shape)\n",
    "            self.output = output_before_dropout * self.dropout_mask\n",
    "        else:\n",
    "           self.output = self.activation(Z)  \n",
    "        #print(f\"set..... self.output {self.output.shape}\")\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, dA,learning_rate, y=None, is_training = False):\n",
    "        \"\"\"Note: Backward propagate through this layer. dA is the derivative of the loss with respect to the output of this layer.\n",
    "        y is the true labels, which is only needed if this is an output layer with softmax activation.\n",
    "        \"\"\"\n",
    "        if is_training and self.dropout_mask is not None:\n",
    "            dA *= self.dropout_mask\n",
    "        #print(f\"self.output {self.output.shape}\")\n",
    "        if self.activation_name == 'softmax':\n",
    "            y_one_hot = np.zeros_like(self.output)\n",
    "            y_one_hot[np.arange(len(y)), y] = 1\n",
    "            # Calculate the derivative of the loss with respect to the softmax inputs\n",
    "            dZ = (self.output - y_one_hot) / len(y)\n",
    "        else:\n",
    "            dZ = dA * self.activation_prime(self.output)\n",
    "        \n",
    "        dA_prev = np.dot(dZ, self.weights.T)\n",
    "        dW = np.dot(self.input.T, dZ)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "        if self.reg_type:\n",
    "            if self.reg_type.upper() == \"L1\":\n",
    "                 #print(\"Using L1 regularization..\") \n",
    "                 weights_reg = self.lambda_reg * np.sign(self.weights)\n",
    "                 biases_reg = self.lambda_reg * np.sign(self.biases)\n",
    "            else:\n",
    "                 #print(\"Using L2 regularization....\") \n",
    "                 weights_reg = self.lambda_reg * self.weights\n",
    "                 biases_reg = self.lambda_reg * self.biases \n",
    "            self.weights -= learning_rate * (dW + weights_reg)\n",
    "            self.biases -= learning_rate * (db + biases_reg)\n",
    "        else:\n",
    "            #print(\"No regularization....\") \n",
    "            self.weights -= learning_rate * dW\n",
    "            self.biases -= learning_rate * db\n",
    "        \n",
    "        return dA_prev\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_prime(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_prime(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "           \n",
    "    def softmax(self,Z):\n",
    "        Z_shift = Z - np.max(Z, axis=1, keepdims=True)\n",
    "        exp_scores = np.exp(Z_shift)\n",
    "        return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-9)  # Softmax activation\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d9388-ca4e-449c-94ed-19bc282fe69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'train_acc':[], 'val_acc':[]}\n",
    "        \n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X, is_training):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X,is_training)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X,is_training=False)\n",
    "        return np.argmax(output, axis=1)\n",
    "        \n",
    "    def cross_entropy_loss(self,y, output):\n",
    "        m = y.shape[0]\n",
    "        log_likelihood = -np.log(output[range(m), y] + 1e-9)\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss      \n",
    "\n",
    "    def train(self, train_data, train_labels, val_data, val_labels, epochs, batch_size, learning_rate,decay_rate= 0.02):\n",
    "      for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(train_data.shape[0])\n",
    "        train_data = train_data[permutation]\n",
    "        train_labels = train_labels[permutation]\n",
    "        # descrese learning_rate after each 10 epochs  \n",
    "        if epoch % 50 == 0:\n",
    "            learning_rate = learning_rate / (1 + decay_rate * epoch)\n",
    "            print(f\"Learning rate :{learning_rate} and decay_rate :{decay_rate}\")\n",
    "        for i in range(0, train_data.shape[0], batch_size):\n",
    "          X_batch = train_data[i:i+batch_size]\n",
    "          y_batch = train_labels[i:i+batch_size]\n",
    "          output = self.forward(X_batch, is_training= True)\n",
    "          self.backward(output, learning_rate, y_batch, is_training=True)\n",
    "        train_loss = self.cross_entropy_loss(train_labels, self.forward(train_data, is_training= True))\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "\n",
    "        val_output = self.forward(val_data, is_training=False)\n",
    "        val_loss = self.cross_entropy_loss(val_labels, val_output)  # Use val_labels directly\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "          \n",
    "        val_accuracy = np.mean(self.predict(val_data) == val_labels)\n",
    "        train_acc = np.mean(self.predict(train_data) == train_labels)\n",
    "        self.history['train_acc'].append(train_acc)\n",
    "        self.history['val_acc'].append(val_accuracy)  \n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "          \n",
    "    def backward(self,output, learning_rate, y_train_batch, is_training=True):\n",
    "        for layer in reversed(self.layers):\n",
    "            #print(layer)\n",
    "            output = layer.backward(output, learning_rate,y_train_batch, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f6941-63e0-4de5-9c04-9671645cbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "# URL and data filename for the Fashion MNIST dataset\n",
    "DATASET_BASE_URL = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "DATASET_BASE_FOLDER = './data/FashionMNIST/raw'\n",
    "DATA_FILENAME = {\n",
    "    'train_images': 'train-images-idx3-ubyte.gz',\n",
    "    'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "    'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "    'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "    \n",
    "# Helper function to download and extract the dataset\n",
    "def download_and_extract(filename, is_image=False):\n",
    "    if not os.path.exists('/'.join([DATASET_BASE_FOLDER, filename])):\n",
    "        os.makedirs(DATASET_BASE_FOLDER, exist_ok=True)\n",
    "        urllib.request.urlretrieve('/'.join([DATASET_BASE_URL, filename]),\n",
    "                                   '/'.join([DATASET_BASE_FOLDER, filename]))\n",
    "    filename = os.path.join(DATASET_BASE_FOLDER, filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        if (is_image):\n",
    "            return np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28) / 255.0\n",
    "        else:\n",
    "            return np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "# Download and extract all files\n",
    "train_images = download_and_extract(DATA_FILENAME['train_images'], is_image=True)\n",
    "train_labels = download_and_extract(DATA_FILENAME['train_labels'])\n",
    "test_images = download_and_extract(DATA_FILENAME['test_images'], is_image=True)\n",
    "test_labels = download_and_extract(DATA_FILENAME['test_labels'])\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "num_train = int(0.8 * len(train_images))\n",
    "train_data, val_data = train_images[:num_train], train_images[num_train:]\n",
    "train_labels, val_labels = train_labels[:num_train], train_labels[num_train:]\n",
    "\n",
    "print(f'Training data shape   : Images - {train_data.shape} | Labels - {train_labels.shape}')\n",
    "print(f'Validation data shape : Images - {val_data.shape} | Labels - {val_labels.shape}')\n",
    "print(f'Test data shape       : Images - {test_images.shape} | Labels - {test_labels.shape}')\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels_one_hot = one_hot(train_labels, 10)\n",
    "val_labels_one_hot = one_hot(val_labels, 10)\n",
    "print(f\"train_labels_one_hot: {train_labels_one_hot.shape}, val_labels_one_hot {val_labels_one_hot.shape}  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9aa2d7-1d48-41c6-84ee-363bef313731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "#dropout_rate=.2\n",
    "nn.add_layer(DenseLayer(input_size, hidden_size, 'relu',reg_type=None, dropout_rate=.5))  #reg_type=\"L2\" does not help\n",
    "#nn.add_layer(DenseLayer(hidden_size, hidden_size, 'relu', reg_type=None, dropout_rate=.2))  #reg_type=\"L2\" does not help and sigmoid does not help, increasing layer does not help\n",
    "nn.add_layer(DenseLayer(hidden_size, output_size, 'softmax', reg_type=None, dropout_rate=None))  #reg_type=\"L2\" does not help\n",
    "nn.train( train_data, train_labels, val_data, val_labels, epochs=epochs, learning_rate=learning_rate, batch_size= 64) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48c9ba-24c4-4e00-8028-00115ce62a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.history['train_loss'], label='Training loss')\n",
    "plt.plot(nn.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37818f5b-b1f1-47ee-a7e5-edabd0881296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.history['train_acc'], label='Training acc')\n",
    "plt.plot(nn.history['val_acc'], label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb885226-0d7e-4b4a-8458-9d11c7fb8222",
   "metadata": {},
   "outputs": [],
   "source": [
    " test_acc = np.mean(nn.predict(test_images) == test_labels)\n",
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
