{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep-kumar-singh/emasters_ee954/blob/main/EE954_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "1. Download the Fashion_MNIST dataset. You can find it on the official Fashion-MNIST website or by using PyTorch's torchvision.datasets module. Split the dataset into training, validation and testing sets. A common split is 80% of the data to train, 10% to validate, and 10% to test scenarios, but you can adjust this as needed. Normalize the images. This involves scaling the pixel values to a range between 0 and 1.\n",
        "\n",
        "2. Implement a MLP for classification. (total 40 marks)\n",
        "    <ol type=\"a\">\n",
        "    <li>Flatten the images into a single dimensional vector before feeding it to the model. (1 marks)</li>\n",
        "    <li>Write a pre-processing module for all the images. (3 marks)</li>\n",
        "    <li>Write the Forward pass from scratch. Use of the inbuilt forward pass function will result in 0 marks for this sub-question. (8 marks)</li>\n",
        "    <li>Write the Backward pass from scratch. Use the inbuilt back propagation function will result in 0 marks for this sub-question (12 marks)</li>\n",
        "    <li>Write the module for cross entropy loss (1 marks)</li>\n",
        "    <li>Experiment with different hyperparameters like number of layers, dropout, objective function, etc. and settle with a combination which performs the best for the given problem. (15 Marks)</li>\n",
        "    </ol>\n",
        "\n",
        "3. Implement a [CNN backbone model](https://www.baeldung.com/cs/neural-network-backbone) using pytorch. (total 40 marks)\n",
        "    <ol type=\"a\">\n",
        "    <li>Build a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by a ReLU activation and a max pooling layer. (10 Marks )</li>\n",
        "    <li>Experiment with different kernel size, number of kernel each layer (keep number of filter same in each layer, double it in each layer etc) and settle with a combination which performs the best for the given problem. (10 Marks)</li>\n",
        "    <li>Try different weight initialization methods (random, Xavier, He) (5 Marks)</li>\n",
        "    <li>After extracting feature from CNN model use MLP for classification (use code from question 2) (15 Marks)</li>\n",
        "    </ol>\n",
        "\n",
        "4. Submit a report clearly explaining how you have built the models, the architecture of the models, learning rate, epochs used for training, evaluation metrics and the instructions for running the models. Compare the performance of the models on the different hyperparameters you tried and justify the observed behavior. (20 Marks)"
      ],
      "metadata": {
        "id": "1t2rodvUWl_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Data Preparation"
      ],
      "metadata": {
        "id": "46TXkkZUi1QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Import Dependencies\n",
        "# ==========================================================\n",
        "\n",
        "# for MLP\n",
        "import numpy as np\n",
        "\n",
        "# for Pytorch based backbone CNN\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "35tdT6raI6mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Data Preparation (using torch)\n",
        "# ==========================================================\n",
        "\n",
        "# PyTorch based flow to prepare and train a backbone CNN\n",
        "########################################################\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),                                                       # this transform does the normalisation of data from [1,255] to [0.0, 1.0]\n",
        ")\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),                                                       # this transform does the normalisation of data from [1,255] to [0.0, 1.0]\n",
        ")\n",
        "\n",
        "# Split the training data into Training and Validation datasets\n",
        "training_data_subset_size = int(0.8 * len(training_data))\n",
        "validate_data_subset_size = len(training_data) - training_data_subset_size\n",
        "training_data_subset, validation_data_subset = random_split(training_data, [training_data_subset_size, validate_data_subset_size])\n",
        "\n",
        "# define batch-size to load data\n",
        "batch_size = 64\n",
        "# define num of epochs to be run for training\n",
        "epochs = 10\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data_subset, batch_size=batch_size)\n",
        "validate_dataloader = DataLoader(validation_data_subset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "2pIm-vbVivZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Data Preparation (Alternate using numpy)\n",
        "# ==========================================================\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "\n",
        "# URL and data filename for the Fashion MNIST dataset\n",
        "DATASET_BASE_URL = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
        "DATASET_BASE_FOLDER = './data/FashionMNIST/raw'\n",
        "DATA_FILENAME = {\n",
        "    'train_images': 'train-images-idx3-ubyte.gz',\n",
        "    'train_labels': 'train-labels-idx1-ubyte.gz',\n",
        "    'test_images': 't10k-images-idx3-ubyte.gz',\n",
        "    'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "# Helper function to download and extract the dataset\n",
        "def download_and_extract(filename, is_image=False):\n",
        "    if not os.path.exists('/'.join([DATASET_BASE_FOLDER, filename])):\n",
        "        os.makedirs(DATASET_BASE_FOLDER, exist_ok=True)\n",
        "        urllib.request.urlretrieve('/'.join([DATASET_BASE_URL, filename]),\n",
        "                                   '/'.join([DATASET_BASE_FOLDER, filename]))\n",
        "    filename = os.path.join(DATASET_BASE_FOLDER, filename)\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        if (is_image):\n",
        "            return np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28) / 255.0\n",
        "        else:\n",
        "            return np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "# Download and extract all files\n",
        "train_images__np = download_and_extract(DATA_FILENAME['train_images'], is_image=True)\n",
        "train_labels__np = download_and_extract(DATA_FILENAME['train_labels'])\n",
        "test_images__np = download_and_extract(DATA_FILENAME['test_images'], is_image=True)\n",
        "test_labels__np = download_and_extract(DATA_FILENAME['test_labels'])\n",
        "\n",
        "# Split the training set into training and validation sets\n",
        "num_train = int(0.8 * len(train_images__np))\n",
        "train_data__np, val_data__np = train_images__np[:num_train], train_images__np[num_train:]\n",
        "train_labels__np, val_labels__np = train_labels__np[:num_train], train_labels__np[num_train:]\n",
        "\n",
        "print(f'Training data shape   : Images - {train_data__np.shape} | Labels - {train_labels__np.shape}')\n",
        "print(f'Validation data shape : Images - {val_data__np.shape} | Labels - {val_labels__np.shape}')\n",
        "print(f'Test data shape       : Images - {test_images__np.shape} | Labels - {test_labels__np.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3p_jdbkjK8_",
        "outputId": "cc07277b-af8f-4a98-8648-cf61c21f471b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape   : Images - (48000, 784) | Labels - (48000,)\n",
            "Validation data shape : Images - (12000, 784) | Labels - (12000,)\n",
            "Test data shape       : Images - (10000, 784) | Labels - (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Select Device for Execution\n",
        "# ==========================================================\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n"
      ],
      "metadata": {
        "id": "G2D4-IAOWbVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f8a8e4-4afb-43f2-e523-4175a39ed1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Define MLP model for classification\n",
        "# ==========================================================\n",
        "\n",
        "# Define a class to represent dense layer\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_dim, output_dim, activation, lambda_reg=0.1, reg_type=None):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
        "        self.biases =  np.zeros((1, output_dim))\n",
        "\n",
        "        self.activation_name = activation\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "        self.output = None\n",
        "        self.input = None\n",
        "\n",
        "        self.reg_type = reg_type\n",
        "\n",
        "        if self.activation_name == 'relu':\n",
        "            self.activation = self.relu\n",
        "            self.activation_prime = self.relu_prime\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            self.activation = self.sigmoid\n",
        "            self.activation_prime = self.sigmoid_prime\n",
        "        elif self.activation_name == 'softmax':\n",
        "            self.activation = self.softmax\n",
        "            self.activation_prime = self.softmax_prime\n",
        "        else:\n",
        "            raise ValueError('activation function is not defined')\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"\"\"DenseLayer(input_dim:{self.input_dim}, output_dim:{self.output_dim}, activation:{self.activation_name})\"\"\"\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        #print(f\"self.input: {self.input.shape} \\n self.weights {self.weights.shape}\")\n",
        "        Z = np.dot(self.input, self.weights) + self.biases\n",
        "        #print(\"Z \", Z.shape)\n",
        "        self.output = self.activation(Z)\n",
        "        #print(f\"set..... self.output {self.output.shape}\")\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dA, learning_rate, y=None):\n",
        "        \"\"\"\n",
        "        Backward propagate through this layer.\n",
        "        dA is the derivative of the loss with respect to the output of this layer.\n",
        "        y is the true labels, which is only needed if this is an output layer with softmax activation.\n",
        "        \"\"\"\n",
        "        #print(f\"self.output {self.output.shape}\")\n",
        "        if self.activation_name == 'softmax':\n",
        "            y_one_hot = np.zeros_like(self.output)\n",
        "            y_one_hot[np.arange(len(y)), y] = 1\n",
        "            # Calculate the derivative of the loss with respect to the softmax inputs\n",
        "            print(len(y))\n",
        "            dZ = (self.output - y_one_hot) / len(y)\n",
        "        else:\n",
        "            dZ = dA * self.activation_prime(self.output)\n",
        "\n",
        "        dA_prev = np.dot(dZ, self.weights.T)\n",
        "        dW = np.dot(self.input.T, dZ)\n",
        "        db = np.sum(dZ, axis=0, keepdims=True)\n",
        "\n",
        "        if self.reg_type:\n",
        "            if self.reg_type.upper() == \"L1\":\n",
        "                 #print(\"Using L1 regularization..\")\n",
        "                 weights_reg = self.lambda_reg * np.sign(self.weights)\n",
        "                 biases_reg = self.lambda_reg * np.sign(self.biases)\n",
        "            else:\n",
        "                 #print(\"Using L2 regularization....\")\n",
        "                 weights_reg = self.lambda_reg * self.weights\n",
        "                 biases_reg = self.lambda_reg * self.biases\n",
        "            self.weights -= learning_rate * (dW + weights_reg)\n",
        "            self.biases -= learning_rate * (db + biases_reg)\n",
        "        else:\n",
        "            #print(\"No regularization....\")\n",
        "            self.weights -= learning_rate * dW\n",
        "            self.biases -= learning_rate * db\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    # ==== Activation functions and their derivatives ====\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_prime(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_prime(self, x):\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "    # Ref https://stackoverflow.com/questions/40575841/numpy-calculate-the-derivative-of-the-softmax-function\n",
        "    def softmax(self,Z):\n",
        "        exp_scores = np.exp(Z)\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # Softmax activation\n",
        "\n",
        "    # The derivative of the cross-entropy loss with respect to the input to the softmax is simply predictions - true_labels\n",
        "    def softmax_prime(self,x):\n",
        "        return 1\n",
        "\n",
        "\n",
        "# Define a class to represent MLP\n",
        "class MLP:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.history = {'train_loss': [], 'val_loss': [], 'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "    def cross_entropy_loss(self,y, output):\n",
        "        m = y.shape[0]\n",
        "        log_likelihood = -np.log(output[range(m), y] + 1e-9)\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def train(self, train_data, train_labels, val_data, val_labels, epochs=10, batch_size=64, learning_rate=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            permutation = np.random.permutation(train_data.shape[0])\n",
        "            train_data = train_data[permutation]\n",
        "            train_labels = train_labels[permutation]\n",
        "            for i in range(0, train_data.shape[0], batch_size):\n",
        "                X_batch = train_data[i:i+batch_size]\n",
        "                y_batch = train_labels[i:i+batch_size]\n",
        "                output = self.forward(X_batch)\n",
        "                self.backward(output, learning_rate, y_batch)\n",
        "            train_loss = self.cross_entropy_loss(train_labels, self.forward(train_data))\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "\n",
        "            val_output = self.forward(val_data)\n",
        "            val_loss = self.cross_entropy_loss(val_labels, val_output)  # Use val_labels directly\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "\n",
        "            val_accuracy = np.mean(self.predict(val_data) == val_labels)\n",
        "            train_acc = np.mean(self.predict(train_data) == train_labels)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_acc'].append(val_accuracy)\n",
        "            print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    def backward(self,output, learning_rate, y_train_batch):\n",
        "        for layer in reversed(self.layers):\n",
        "            #print(layer)\n",
        "            output = layer.backward(output, learning_rate,y_train_batch)"
      ],
      "metadata": {
        "id": "9TOGxE8qYEYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "# For Debugging - CNN Layer Output Size\n",
        "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "\n",
        "# input = torch.randn(784, 3, 3)\n",
        "\n",
        "# # m = nn.Conv2d(392, 784, (2, 2), stride=(1, 1), padding=(1, 1))\n",
        "# m = nn.MaxPool2d((2, 2))\n",
        "# output = m(input)\n",
        "\n",
        "# print(input.size(), output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W966wQ6JROgL",
        "outputId": "f87141f8-192c-4930-be78-3c716a6162ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784, 3, 3]) torch.Size([784, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Define backbone CNN model for feature extraction\n",
        "# ==========================================================\n",
        "\n",
        "class BackboneNeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a small CNN model consisting of 5 convolution layers. Each convolution\n",
        "    layer would be followed by a ReLU activation and a max pooling layer.\n",
        "\n",
        "    Dense network with 2 layers, with a ReLU activation after first layer. This\n",
        "    layer can be used ONLY when testing CNN model in isolation. After extracting\n",
        "    feature from CNN model, use MLP for classification.\n",
        "\n",
        "    NOTE: Dense network is not used, if `backbone_only` is `True`.\n",
        "\n",
        "    REFERENCES:\n",
        "        1. https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
        "        nn.Sequential()\n",
        "\n",
        "        2. https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
        "        nn.Conv2d(\n",
        "            in_channels = number of layers in input images. Grayscale or monochrome images have 1 in_channels\n",
        "            out_channels = number of channels in the output produced. This is a hyperparameter, which signifies the number of kernels\n",
        "            kernel_size = `(m,n)` for a kernel/filter dimension, or simply n for a square (n,n) kernel/filter dimension\n",
        "            stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None are other properties with default values\n",
        "        )\n",
        "\n",
        "        3. https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU\n",
        "        nn.ReLU()\n",
        "\n",
        "        4. https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n",
        "        nn.MaxPool2d(\n",
        "            kernel_size = `(m,n)` for a kernel/filter dimension, or simply n for a square (n,n) kernel/filter dimension\n",
        "            stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False are other properties with default values\n",
        "        )\n",
        "\n",
        "        5. https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "        nn.Linear(\n",
        "            in_features = size of each input sample\n",
        "            out_features = size of each output sample\n",
        "            bias=True, device=None, dtype=None are other properties with default values\n",
        "        )\n",
        "\n",
        "        6. https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax\n",
        "        nn.Softmax()\n",
        "\n",
        "        7. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "        Training a Classifier\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructor for the CNN Model.\n",
        "    #\n",
        "    # NOTE: If `backbone_only` is `True`, dense network is not used.\n",
        "    def __init__(self, backbone_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # this property helps the CNN transition from a full-fledged network to a backbone CNN\n",
        "        # the default value is False - meaning an object of this class can be used to predict the labels for Fashion-MNIST dataset\n",
        "        # if the value is set to True - an object of this class will return the flattened output from conv layers - thus acting as a backbone\n",
        "        self.backbone_only = backbone_only\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # CNN model consisting of 5 convolution layers with each convolution\n",
        "        # layer followed by a ReLU activation and a max pooling layer.\n",
        "        self.convolutional_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(1, 49, (3, 3), stride=(1, 1), padding=(1, 1)),    # input = (1,28,28), output = (49, 28, 28)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),                                       # input = (49, 28, 28), output = (49, 14, 14)\n",
        "            nn.Conv2d(49, 98, (2, 2), stride=(1, 1), padding=(1,1)),    # input = (49, 14, 14), output = (98, 15, 15)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),                                       # input = (98, 15, 15), output = (98, 7, 7)\n",
        "            nn.Conv2d(98, 196, (2, 2), stride=(1, 1), padding=(1,1)),   # input = (98, 7, 7), output = (196, 8, 8)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),                                       # input = (196, 8, 8), output = (196, 4, 4)\n",
        "            nn.Conv2d(196, 392, (2, 2), stride=(1, 1), padding=(1, 1)), # input = (196, 4, 4), output = (392, 5, 5)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),                                       # input = (392, 5, 5), output = (392, 2, 2)\n",
        "            nn.Conv2d(392, 784, (2, 2), stride=(1, 1), padding=(1, 1)), # input = (392, 2, 2), output = (784, 3, 3)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2))                                        # input = (784, 3, 3), output = (784,1,1)\n",
        "        )\n",
        "\n",
        "        # Dense network with 2 layers with a ReLU activation after first layer.\n",
        "        # This layer can be used ONLY when testing CNN model in isolation.\n",
        "        #\n",
        "        # NOTE: Dense network is not used, if `backbone_only` is `True`.\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    # Forward pass for CNN Model\n",
        "    def forward(self, x):\n",
        "        print(f'CNN Input Size: {x.size()}')\n",
        "        x1 = self.convolutional_relu_stack(x)\n",
        "        print(f'CNN Output Size: {x1.size()}')\n",
        "        x2 = self.flatten(x1)\n",
        "        print(f'Flatten Size: {x2.size()}')\n",
        "\n",
        "        if self.backbone_only:  # return the flattened tensor containing feature extraction data\n",
        "            return x2\n",
        "\n",
        "        # default behaviour is to return the predicted labels\n",
        "        x3 = self.linear_relu_stack(x2)\n",
        "        return x3"
      ],
      "metadata": {
        "id": "mZBOeengJfSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Data Preparation\n",
        "# ==========================================================\n",
        "\n",
        "# PyTorch based flow to prepare and train a backbone CNN\n",
        "########################################################\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Split the training data into Training and Validation datasets\n",
        "training_data_subset_size = int(0.8 * len(training_data))\n",
        "validate_data_subset_size = len(training_data) - training_data_subset_size\n",
        "training_data_subset, validation_data_subset = random_split(training_data, [training_data_subset_size, validate_data_subset_size])\n",
        "\n",
        "print(f'Complete Training Data: {training_data.size}')\n",
        "\n",
        "# define batch-size to load data\n",
        "batch_size = 64\n",
        "# define num of epochs to be run for training\n",
        "epochs = 10\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data_subset, batch_size=batch_size)\n",
        "validate_dataloader = DataLoader(validation_data_subset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# for X, y in test_dataloader:\n",
        "#     print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "#     print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "#     break\n",
        "\n",
        "# create model instance\n",
        "model = BackboneNeuralNetwork().to(device)\n",
        "# print(model)\n",
        "\n",
        "# define loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "# forward pass implementation\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "# validation implementation\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    validation_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            validation_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    validation_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Validation Phase: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
        "\n",
        "# testing implementation\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Phase: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# Train and Validate the CNN\n",
        "print(\"Training the backbone CNN model\")\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    validate(validate_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch CNN backbone model state to model.pth\")\n",
        "\n",
        "# Load the model to perform testing on the trained variables\n",
        "model = BackboneNeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# Test with the model\n",
        "test(test_dataloader, model, loss_fn)\n",
        "\n",
        "#####################################\n",
        "# Once the backbone model is prepared, trained, and tested,\n",
        "# start the integration of backbone model with custom MLP\n",
        "\n",
        "# define the MLP architecture\n",
        "input_size = 28 * 28\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "\n",
        "# create the instance of MLP\n",
        "mlp = MLP()\n",
        "mlp.add_layer(DenseLayer(input_size, hidden_size, 'relu'))      #reg_type=\"L2\" does not help\n",
        "mlp.add_layer(DenseLayer(hidden_size, output_size, 'softmax'))  #reg_type=\"L2\" does not help\n",
        "\n",
        "# train the MLP using features extracted from pre-trained backbone\n",
        "def feature_extraction(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    features = np.empty((size, 28*28), dtype=np.float64)\n",
        "    labels = np.empty((size), dtype=np.int64)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            features[(batch*batch_size):((batch+1)*batch_size)] = pred.numpy()  # extract the features in numpy arrays\n",
        "            labels[(batch*batch_size):((batch+1)*batch_size)] = y.numpy()       # extract the features in numpy arrays\n",
        "    return features, labels\n",
        "\n",
        "# create the instance of `backbone CNN model`\n",
        "backbone_model = BackboneNeuralNetwork(backbone_only=True).to(device)\n",
        "backbone_model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# extract the features using backbone CNN\n",
        "classifier_train_data, classifier_train_labels = feature_extraction(train_dataloader, backbone_model)\n",
        "classifier_validation_data, classifier_validation_labels = feature_extraction(validate_dataloader, backbone_model)\n",
        "\n",
        "# let the MLP classify the data now based on feature-extracted dataset\n",
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "print(\"Training the classigication MLP model\")\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    mlp.train(classifier_train_data, classifier_train_labels, classifier_validation_data, classifier_validation_labels)\n"
      ],
      "metadata": {
        "id": "BnkLy-PyIvvs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}